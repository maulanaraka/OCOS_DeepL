{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e494b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ultralytics mss opencv-python matplotlib pygetwindow pyautogui torch torchvision Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c386fa",
   "metadata": {},
   "source": [
    "## RESNET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dab79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import pygetwindow as gw\n",
    "import mss\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4f4234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\maula\\AppData\\Local\\Temp\\ipykernel_32536\\419498200.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\car_door_model_new.pt\", map_location=DEVICE))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'torchvision.models.resnet.ResNet'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     17\u001b[39m model = models.resnet18(pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     18\u001b[39m model.fc = nn.Sequential(\n\u001b[32m     19\u001b[39m     nn.Linear(model.fc.in_features, \u001b[32m256\u001b[39m),\n\u001b[32m     20\u001b[39m     nn.ReLU(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     nn.Sigmoid()\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mSchool\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mSemester 8\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mPENGANTAR DEEP LEARNING\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUTS\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcar_door_model_new.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m model.to(DEVICE)\n\u001b[32m     27\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2516\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2479\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[32m   2480\u001b[39m \n\u001b[32m   2481\u001b[39m \u001b[33;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2513\u001b[39m \u001b[33;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[32m   2514\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[32m-> \u001b[39m\u001b[32m2516\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   2517\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2518\u001b[39m     )\n\u001b[32m   2520\u001b[39m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m   2521\u001b[39m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] = []\n",
      "\u001b[31mTypeError\u001b[39m: Expected state_dict to be dict-like, got <class 'torchvision.models.resnet.ResNet'>."
     ]
    }
   ],
   "source": [
    "# === Model & Device ===\n",
    "IMG_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CLASSES = [\n",
    "    \"front_left_door_closed\", \"front_left_door_open\",\n",
    "    \"front_right_door_closed\", \"front_right_door_open\",\n",
    "    \"hood_closed\", \"hood_open\",\n",
    "    \"rear_left_door_closed\", \"rear_left_door_open\",\n",
    "    \"rear_right_door_closed\", \"rear_right_door_open\"\n",
    "]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, len(CLASSES)),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model.load_state_dict(torch.load(r\"D:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\car_door_model_new.pt\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === Open Browser ===\n",
    "url = \"http://103.233.100.26:8080/\"\n",
    "webbrowser.open(url)\n",
    "\n",
    "print(\"‚åõ Waiting for the browser window to appear...\")\n",
    "\n",
    "window_title = None\n",
    "while window_title is None:\n",
    "    windows = gw.getWindowsWithTitle(url)\n",
    "    if not windows:\n",
    "        windows = [w for w in gw.getWindowsWithTitle(\" - Google Chrome\") if \"Car Control Simulation\" in w.title]\n",
    "    if windows:\n",
    "        win = windows[0]\n",
    "        window_title = win.title\n",
    "    else:\n",
    "        time.sleep(1)  # wait and try again\n",
    "\n",
    "print(f\"‚úÖ Found window: {window_title}\")\n",
    "win = gw.getWindowsWithTitle(window_title)[0]\n",
    "\n",
    "# === Inference Loop ===\n",
    "with mss.mss() as sct:\n",
    "    while True:\n",
    "        bbox = {\n",
    "            \"top\": win.top,\n",
    "            \"left\": win.left,\n",
    "            \"width\": win.width,\n",
    "            \"height\": win.height\n",
    "        }\n",
    "        screen = sct.grab(bbox)\n",
    "        img = Image.frombytes(\"RGB\", screen.size, screen.rgb)\n",
    "\n",
    "        # Preprocess\n",
    "        input_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            preds = (outputs > 0.5).float().squeeze()\n",
    "\n",
    "        # Draw predictions on image\n",
    "        frame = np.array(img)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        y = 30\n",
    "        for i, cls in enumerate(CLASSES):\n",
    "            if preds[i] == 1:\n",
    "                # Determine status and symbol\n",
    "                status = \"open\"\n",
    "                color = (0, 255, 0)  # Green for open\n",
    "                symbol = \"O\"  # Green check for open\n",
    "                if \"closed\" in cls:\n",
    "                    status = \"closed\"\n",
    "                    color = (0, 0, 255)  # Red for closed\n",
    "                    symbol = \"X\"  # Red cross for closed\n",
    "\n",
    "                # Prepare the label to display (e.g., hood: open/closed)\n",
    "                text = f\"{' '.join(cls.split('_')[:-1])}: {status}\"\n",
    "\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                font_scale = 0.7\n",
    "                thickness = 2\n",
    "\n",
    "                # Text size to calculate the box width\n",
    "                text_size, _ = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "                text_w, text_h = text_size\n",
    "                x, y_pos = 10, y\n",
    "\n",
    "                # Draw filled rectangle (background box)\n",
    "                cv2.rectangle(frame, (x - 5, y_pos - text_h - 5), (x + text_w + 5, y_pos + 5), (0, 0, 0), -1)\n",
    "\n",
    "                # Draw the status text\n",
    "                cv2.putText(frame, text, (x, y_pos), font, font_scale, (255, 255, 255), thickness)\n",
    "\n",
    "                # Draw the symbol (‚úì or ‚úò) in front of the text\n",
    "                cv2.putText(frame, symbol, (x + text_w + 10, y_pos), font, font_scale, color, thickness)\n",
    "\n",
    "                # Adjust vertical position for the next line\n",
    "                y += text_h + 15\n",
    "\n",
    "        # Show result\n",
    "        cv2.imshow(\"üöó Predicted Car State\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c38d5",
   "metadata": {},
   "source": [
    "## YOLO OBJECT DETECTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa63c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygetwindow as gw\n",
    "from ultralytics import YOLO\n",
    "import mss\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c9da172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left': 0, 'top': 0, 'width': 3840, 'height': 1200}\n",
      "{'left': 0, 'top': 0, 'width': 1920, 'height': 1200}\n",
      "{'left': 1920, 'top': 116, 'width': 1920, 'height': 1080}\n"
     ]
    }
   ],
   "source": [
    "# Debug monitor info\n",
    "with mss.mss() as sct:\n",
    "    for monitor in sct.monitors:\n",
    "        print(monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c7b2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load your custom YOLO model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mSchool\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mSemester 8\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mPENGANTAR DEEP LEARNING\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUTS\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcar_door_model.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# === Open Browser ===\u001b[39;00m\n\u001b[32m      5\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttp://103.233.100.26:8080/\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\.venv\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[39m, in \u001b[36mYOLO.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m = new_instance.\u001b[34m__dict__\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:148\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m._new(model, task=task, verbose=verbose)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:290\u001b[39m, in \u001b[36mModel._load\u001b[39m\u001b[34m(self, weights, task)\u001b[39m\n\u001b[32m    287\u001b[39m weights = checks.check_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Path(weights).suffix == \u001b[33m\"\u001b[39m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.ckpt = \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m.task = \u001b[38;5;28mself\u001b[39m.model.args[\u001b[33m\"\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.overrides = \u001b[38;5;28mself\u001b[39m.model.args = \u001b[38;5;28mself\u001b[39m._reset_ckpt_args(\u001b[38;5;28mself\u001b[39m.model.args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1041\u001b[39m, in \u001b[36mattempt_load_one_weight\u001b[39m\u001b[34m(weight, device, inplace, fuse)\u001b[39m\n\u001b[32m   1039\u001b[39m ckpt, weight = torch_safe_load(weight)  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[32m   1040\u001b[39m args = {**DEFAULT_CFG_DICT, **(ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m model = (ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m).to(device).float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n\u001b[32m   1043\u001b[39m \u001b[38;5;66;03m# Model compatibility updates\u001b[39;00m\n\u001b[32m   1044\u001b[39m model.args = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m args.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m DEFAULT_CFG_KEYS}  \u001b[38;5;66;03m# attach args to model\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'model'"
     ]
    }
   ],
   "source": [
    "# Load your custom YOLO model\n",
    "model = YOLO(r\"D:\\School\\Semester 8\\PENGANTAR DEEP LEARNING\\UTS\\runs\\detect\\car-object-detection\\weights\\best.pt\")\n",
    "\n",
    "# === Open Browser ===\n",
    "url = \"http://103.233.100.26:8080/\"\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "print(\"‚åõ Waiting for the browser window to appear...\")\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "window_title = None\n",
    "while window_title is None:\n",
    "    windows = gw.getWindowsWithTitle(url)\n",
    "    if not windows:\n",
    "        windows = [w for w in gw.getWindowsWithTitle(\" - Google Chrome\") if \"Car Control Simulation\" in w.title]\n",
    "    if windows:\n",
    "        win = windows[0]\n",
    "        window_title = win.title\n",
    "    else:\n",
    "        time.sleep(1)  # wait and try again\n",
    "\n",
    "print(f\"‚úÖ Found window: {window_title}\")\n",
    "win = gw.getWindowsWithTitle(window_title)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5684b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Screen BBOX for mss: (186, 268, 1737, 1049)\n"
     ]
    }
   ],
   "source": [
    "# Get canvas position and size\n",
    "\n",
    "def get_canvas_bbox(driver):\n",
    "    # Get canvas position and size relative to browser viewport\n",
    "    canvas = driver.find_element(By.TAG_NAME, \"canvas\")\n",
    "    canvas_rect = driver.execute_script(\"\"\"\n",
    "        const rect = arguments[0].getBoundingClientRect();\n",
    "        return {\n",
    "            x: rect.left,\n",
    "            y: rect.top,\n",
    "            width: rect.width,\n",
    "            height: rect.height\n",
    "        };\n",
    "    \"\"\", canvas)\n",
    "\n",
    "    # Get the window's screen position\n",
    "    import pygetwindow as gw\n",
    "    win_title_part = \"Car Control\"\n",
    "    win = next((w for w in gw.getWindowsWithTitle(\"\") if win_title_part in w.title), None)\n",
    "    if not win:\n",
    "        raise Exception(f\"Window with title containing '{win_title_part}' not found.\")\n",
    "    win_x, win_y = win.left, win.top\n",
    "    \n",
    "    offset_y = 180  # Browser offset y axis\n",
    "    offset_x = 2  # Browser offset x axis\n",
    "\n",
    "    # Handle DPI scaling\n",
    "    dpi_scale = driver.execute_script(\"return window.devicePixelRatio\")\n",
    "    dpi_scale = dpi_scale * 1.01\n",
    "\n",
    "    # Convert to absolute screen coordinates\n",
    "    left = int(win_x + canvas_rect['x'] * dpi_scale + offset_x)\n",
    "    top = int(win_y + canvas_rect['y'] * dpi_scale + offset_y)\n",
    "    right = int(left + canvas_rect['width'] * dpi_scale)\n",
    "    bottom = int(top + canvas_rect['height'] * dpi_scale)\n",
    "\n",
    "    print(f\"üñ•Ô∏è Screen BBOX for mss: ({left}, {top}, {right}, {bottom})\")\n",
    "    return (left, top, right, bottom)\n",
    "\n",
    "bbox = get_canvas_bbox(driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f5b5f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 (no detections), 78.5ms\n",
      "Speed: 5.3ms preprocess, 78.5ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 18.9ms\n",
      "Speed: 1.7ms preprocess, 18.9ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 15.8ms\n",
      "Speed: 2.0ms preprocess, 15.8ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 15.7ms\n",
      "Speed: 1.7ms preprocess, 15.7ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 15.9ms\n",
      "Speed: 1.6ms preprocess, 15.9ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 17.9ms\n",
      "Speed: 2.3ms preprocess, 17.9ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 17.2ms\n",
      "Speed: 2.0ms preprocess, 17.2ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 24.8ms\n",
      "Speed: 1.5ms preprocess, 24.8ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 15.0ms\n",
      "Speed: 4.9ms preprocess, 15.0ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 18.8ms\n",
      "Speed: 3.8ms preprocess, 18.8ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 14.4ms\n",
      "Speed: 3.4ms preprocess, 14.4ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 19.2ms\n",
      "Speed: 1.9ms preprocess, 19.2ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 16.5ms\n",
      "Speed: 2.1ms preprocess, 16.5ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 15.9ms\n",
      "Speed: 1.6ms preprocess, 15.9ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 17.5ms\n",
      "Speed: 1.5ms preprocess, 17.5ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 17.1ms\n",
      "Speed: 1.8ms preprocess, 17.1ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 14.8ms\n",
      "Speed: 1.6ms preprocess, 14.8ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 16.6ms\n",
      "Speed: 3.1ms preprocess, 16.6ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 21.4ms\n",
      "Speed: 2.1ms preprocess, 21.4ms inference, 2.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 21.2ms\n",
      "Speed: 5.6ms preprocess, 21.2ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 17.0ms\n",
      "Speed: 1.7ms preprocess, 17.0ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 17.8ms\n",
      "Speed: 1.6ms preprocess, 17.8ms inference, 4.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 19.3ms\n",
      "Speed: 4.5ms preprocess, 19.3ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 19.0ms\n",
      "Speed: 1.6ms preprocess, 19.0ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 22.8ms\n",
      "Speed: 3.6ms preprocess, 22.8ms inference, 3.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 19.6ms\n",
      "Speed: 3.6ms preprocess, 19.6ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 18.8ms\n",
      "Speed: 1.6ms preprocess, 18.8ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 19.0ms\n",
      "Speed: 1.9ms preprocess, 19.0ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 22.2ms\n",
      "Speed: 3.1ms preprocess, 22.2ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.3ms\n",
      "Speed: 6.7ms preprocess, 37.3ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.8ms\n",
      "Speed: 2.9ms preprocess, 35.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 41.4ms\n",
      "Speed: 4.1ms preprocess, 41.4ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 3.0ms preprocess, 35.7ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 3.3ms preprocess, 35.7ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.5ms\n",
      "Speed: 2.8ms preprocess, 37.5ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.0ms\n",
      "Speed: 2.9ms preprocess, 37.0ms inference, 4.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.0ms\n",
      "Speed: 2.7ms preprocess, 35.0ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 83.1ms\n",
      "Speed: 5.4ms preprocess, 83.1ms inference, 3.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 78.1ms\n",
      "Speed: 4.4ms preprocess, 78.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 77.6ms\n",
      "Speed: 7.0ms preprocess, 77.6ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.1ms\n",
      "Speed: 3.3ms preprocess, 36.1ms inference, 3.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 38.6ms\n",
      "Speed: 4.9ms preprocess, 38.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 46.3ms\n",
      "Speed: 6.0ms preprocess, 46.3ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.9ms\n",
      "Speed: 3.4ms preprocess, 36.9ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.6ms\n",
      "Speed: 3.0ms preprocess, 35.6ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.1ms\n",
      "Speed: 3.0ms preprocess, 36.1ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.6ms\n",
      "Speed: 3.7ms preprocess, 37.6ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.3ms\n",
      "Speed: 2.8ms preprocess, 36.3ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.8ms\n",
      "Speed: 3.1ms preprocess, 35.8ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 77.9ms\n",
      "Speed: 5.1ms preprocess, 77.9ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 77.9ms\n",
      "Speed: 4.7ms preprocess, 77.9ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 75.6ms\n",
      "Speed: 5.5ms preprocess, 75.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 75.0ms\n",
      "Speed: 4.3ms preprocess, 75.0ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.4ms\n",
      "Speed: 3.3ms preprocess, 35.4ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.9ms\n",
      "Speed: 4.2ms preprocess, 35.9ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.3ms\n",
      "Speed: 3.1ms preprocess, 34.3ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.3ms\n",
      "Speed: 3.2ms preprocess, 34.3ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 83.3ms\n",
      "Speed: 4.5ms preprocess, 83.3ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 84.1ms\n",
      "Speed: 6.4ms preprocess, 84.1ms inference, 4.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 77.8ms\n",
      "Speed: 5.1ms preprocess, 77.8ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 75.5ms\n",
      "Speed: 5.1ms preprocess, 75.5ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 74.8ms\n",
      "Speed: 4.1ms preprocess, 74.8ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.0ms\n",
      "Speed: 3.0ms preprocess, 35.0ms inference, 3.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.3ms\n",
      "Speed: 2.9ms preprocess, 34.3ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.6ms\n",
      "Speed: 4.4ms preprocess, 34.6ms inference, 3.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.4ms\n",
      "Speed: 3.5ms preprocess, 34.4ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.2ms\n",
      "Speed: 2.9ms preprocess, 34.2ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.7ms\n",
      "Speed: 4.8ms preprocess, 34.7ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.7ms\n",
      "Speed: 2.9ms preprocess, 34.7ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.3ms\n",
      "Speed: 3.3ms preprocess, 35.3ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.6ms\n",
      "Speed: 3.0ms preprocess, 35.6ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.3ms\n",
      "Speed: 2.9ms preprocess, 35.3ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.4ms\n",
      "Speed: 4.0ms preprocess, 34.4ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 82.6ms\n",
      "Speed: 5.1ms preprocess, 82.6ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 81.4ms\n",
      "Speed: 5.9ms preprocess, 81.4ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 78.6ms\n",
      "Speed: 5.4ms preprocess, 78.6ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 76.7ms\n",
      "Speed: 5.5ms preprocess, 76.7ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.1ms\n",
      "Speed: 4.2ms preprocess, 37.1ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.6ms\n",
      "Speed: 3.7ms preprocess, 36.6ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.8ms\n",
      "Speed: 4.3ms preprocess, 35.8ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 82.1ms\n",
      "Speed: 5.5ms preprocess, 82.1ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 81.5ms\n",
      "Speed: 5.6ms preprocess, 81.5ms inference, 4.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 79.2ms\n",
      "Speed: 4.4ms preprocess, 79.2ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 76.3ms\n",
      "Speed: 4.3ms preprocess, 76.3ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.5ms\n",
      "Speed: 4.9ms preprocess, 37.5ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.3ms\n",
      "Speed: 3.0ms preprocess, 37.3ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 38.9ms\n",
      "Speed: 3.4ms preprocess, 38.9ms inference, 2.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.6ms\n",
      "Speed: 4.1ms preprocess, 37.6ms inference, 2.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.8ms\n",
      "Speed: 3.8ms preprocess, 36.8ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.9ms\n",
      "Speed: 3.5ms preprocess, 36.9ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.7ms\n",
      "Speed: 2.8ms preprocess, 36.7ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 38.1ms\n",
      "Speed: 4.6ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 38.0ms\n",
      "Speed: 3.2ms preprocess, 38.0ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.7ms\n",
      "Speed: 7.2ms preprocess, 37.7ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.3ms\n",
      "Speed: 2.7ms preprocess, 37.3ms inference, 3.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.2ms\n",
      "Speed: 2.8ms preprocess, 37.2ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 42.2ms\n",
      "Speed: 3.8ms preprocess, 42.2ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 74.2ms\n",
      "Speed: 6.3ms preprocess, 74.2ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 3.0ms preprocess, 35.7ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.7ms\n",
      "Speed: 3.2ms preprocess, 36.7ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.3ms\n",
      "Speed: 2.8ms preprocess, 36.3ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.3ms\n",
      "Speed: 2.9ms preprocess, 35.3ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.9ms\n",
      "Speed: 3.6ms preprocess, 35.9ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.3ms\n",
      "Speed: 5.5ms preprocess, 36.3ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 2.9ms preprocess, 35.7ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.0ms\n",
      "Speed: 2.8ms preprocess, 36.0ms inference, 2.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.9ms\n",
      "Speed: 3.0ms preprocess, 35.9ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 3.6ms preprocess, 35.7ms inference, 4.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.6ms\n",
      "Speed: 4.9ms preprocess, 36.6ms inference, 2.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.0ms\n",
      "Speed: 4.2ms preprocess, 36.0ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 3.0ms preprocess, 35.7ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.1ms\n",
      "Speed: 4.4ms preprocess, 37.1ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.3ms\n",
      "Speed: 3.5ms preprocess, 35.3ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.9ms\n",
      "Speed: 2.9ms preprocess, 35.9ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 2.9ms preprocess, 35.7ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.5ms\n",
      "Speed: 2.9ms preprocess, 35.5ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.1ms\n",
      "Speed: 2.9ms preprocess, 36.1ms inference, 2.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.4ms\n",
      "Speed: 2.8ms preprocess, 36.4ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.0ms\n",
      "Speed: 2.8ms preprocess, 37.0ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.3ms\n",
      "Speed: 2.9ms preprocess, 37.3ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.8ms\n",
      "Speed: 4.2ms preprocess, 36.8ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.8ms\n",
      "Speed: 3.1ms preprocess, 36.8ms inference, 3.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.9ms\n",
      "Speed: 2.8ms preprocess, 36.9ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.1ms\n",
      "Speed: 5.3ms preprocess, 37.1ms inference, 2.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.2ms\n",
      "Speed: 7.4ms preprocess, 36.2ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.5ms\n",
      "Speed: 2.8ms preprocess, 36.5ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.2ms\n",
      "Speed: 3.0ms preprocess, 36.2ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.2ms\n",
      "Speed: 3.4ms preprocess, 36.2ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.5ms\n",
      "Speed: 4.2ms preprocess, 36.5ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.4ms\n",
      "Speed: 2.8ms preprocess, 36.4ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.0ms\n",
      "Speed: 4.1ms preprocess, 36.0ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.5ms\n",
      "Speed: 2.8ms preprocess, 36.5ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.8ms\n",
      "Speed: 3.0ms preprocess, 36.8ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.2ms\n",
      "Speed: 3.4ms preprocess, 36.2ms inference, 3.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.8ms\n",
      "Speed: 7.1ms preprocess, 37.8ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.0ms\n",
      "Speed: 2.9ms preprocess, 36.0ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.3ms\n",
      "Speed: 4.5ms preprocess, 36.3ms inference, 3.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.7ms\n",
      "Speed: 3.1ms preprocess, 36.7ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.1ms\n",
      "Speed: 3.2ms preprocess, 37.1ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.2ms\n",
      "Speed: 4.3ms preprocess, 36.2ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.6ms\n",
      "Speed: 3.7ms preprocess, 36.6ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.8ms\n",
      "Speed: 2.9ms preprocess, 35.8ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.5ms\n",
      "Speed: 5.3ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.9ms\n",
      "Speed: 6.2ms preprocess, 35.9ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 83.0ms\n",
      "Speed: 5.3ms preprocess, 83.0ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 82.7ms\n",
      "Speed: 7.1ms preprocess, 82.7ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 76.5ms\n",
      "Speed: 4.4ms preprocess, 76.5ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 76.0ms\n",
      "Speed: 5.2ms preprocess, 76.0ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 76.8ms\n",
      "Speed: 4.2ms preprocess, 76.8ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.5ms\n",
      "Speed: 3.2ms preprocess, 34.5ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.1ms\n",
      "Speed: 4.1ms preprocess, 35.1ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.8ms\n",
      "Speed: 4.1ms preprocess, 33.8ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.2ms\n",
      "Speed: 2.9ms preprocess, 35.2ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.3ms\n",
      "Speed: 4.2ms preprocess, 34.3ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.1ms\n",
      "Speed: 6.4ms preprocess, 36.1ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.5ms\n",
      "Speed: 3.2ms preprocess, 34.5ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.2ms\n",
      "Speed: 5.3ms preprocess, 35.2ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.3ms\n",
      "Speed: 2.8ms preprocess, 34.3ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.9ms\n",
      "Speed: 3.6ms preprocess, 34.9ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.8ms\n",
      "Speed: 3.5ms preprocess, 33.8ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.1ms\n",
      "Speed: 4.9ms preprocess, 34.1ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.9ms\n",
      "Speed: 4.4ms preprocess, 34.9ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.3ms\n",
      "Speed: 3.6ms preprocess, 35.3ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.3ms\n",
      "Speed: 4.2ms preprocess, 34.3ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.6ms\n",
      "Speed: 4.2ms preprocess, 34.6ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.0ms\n",
      "Speed: 2.8ms preprocess, 34.0ms inference, 2.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.5ms\n",
      "Speed: 4.6ms preprocess, 34.5ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 77.1ms\n",
      "Speed: 5.7ms preprocess, 77.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 77.1ms\n",
      "Speed: 5.4ms preprocess, 77.1ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 75.4ms\n",
      "Speed: 6.4ms preprocess, 75.4ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 75.0ms\n",
      "Speed: 5.5ms preprocess, 75.0ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.0ms\n",
      "Speed: 4.5ms preprocess, 35.0ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 4.3ms preprocess, 35.7ms inference, 3.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.2ms\n",
      "Speed: 5.4ms preprocess, 35.2ms inference, 3.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.5ms\n",
      "Speed: 3.6ms preprocess, 34.5ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.1ms\n",
      "Speed: 3.6ms preprocess, 35.1ms inference, 1.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.3ms\n",
      "Speed: 4.7ms preprocess, 34.3ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.5ms\n",
      "Speed: 3.8ms preprocess, 35.5ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.2ms\n",
      "Speed: 3.0ms preprocess, 35.2ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.4ms\n",
      "Speed: 2.7ms preprocess, 34.4ms inference, 3.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.9ms\n",
      "Speed: 4.8ms preprocess, 34.9ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.4ms\n",
      "Speed: 8.5ms preprocess, 34.4ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.5ms\n",
      "Speed: 3.5ms preprocess, 34.5ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.8ms\n",
      "Speed: 2.9ms preprocess, 36.8ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.0ms\n",
      "Speed: 3.5ms preprocess, 37.0ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 1 rear_left_door_open, 35.3ms\n",
      "Speed: 4.6ms preprocess, 35.3ms inference, 4.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 1 rear_left_door_open, 34.4ms\n",
      "Speed: 3.6ms preprocess, 34.4ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 1 rear_left_door_open, 34.5ms\n",
      "Speed: 3.8ms preprocess, 34.5ms inference, 7.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 1 rear_left_door_open, 34.3ms\n",
      "Speed: 2.9ms preprocess, 34.3ms inference, 3.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 1 rear_left_door_open, 34.0ms\n",
      "Speed: 3.9ms preprocess, 34.0ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.9ms\n",
      "Speed: 4.6ms preprocess, 33.9ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.8ms\n",
      "Speed: 2.9ms preprocess, 35.8ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.7ms\n",
      "Speed: 2.7ms preprocess, 33.7ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.5ms\n",
      "Speed: 3.4ms preprocess, 33.5ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.5ms\n",
      "Speed: 9.8ms preprocess, 34.5ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.7ms\n",
      "Speed: 3.7ms preprocess, 33.7ms inference, 2.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.6ms\n",
      "Speed: 3.3ms preprocess, 33.6ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.4ms\n",
      "Speed: 2.9ms preprocess, 33.4ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.9ms\n",
      "Speed: 4.5ms preprocess, 34.9ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.6ms\n",
      "Speed: 3.5ms preprocess, 33.6ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.9ms\n",
      "Speed: 3.6ms preprocess, 34.9ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.4ms\n",
      "Speed: 3.4ms preprocess, 35.4ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.2ms\n",
      "Speed: 2.8ms preprocess, 35.2ms inference, 1.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.8ms\n",
      "Speed: 2.8ms preprocess, 33.8ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.8ms\n",
      "Speed: 3.1ms preprocess, 34.8ms inference, 3.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.7ms\n",
      "Speed: 2.9ms preprocess, 33.7ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.1ms\n",
      "Speed: 2.8ms preprocess, 33.1ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.3ms\n",
      "Speed: 2.8ms preprocess, 33.3ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.9ms\n",
      "Speed: 3.0ms preprocess, 33.9ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.5ms\n",
      "Speed: 2.9ms preprocess, 33.5ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.3ms\n",
      "Speed: 3.4ms preprocess, 33.3ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.4ms\n",
      "Speed: 3.1ms preprocess, 33.4ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.8ms\n",
      "Speed: 3.6ms preprocess, 33.8ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.4ms\n",
      "Speed: 9.7ms preprocess, 33.4ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.7ms\n",
      "Speed: 2.9ms preprocess, 34.7ms inference, 3.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.5ms\n",
      "Speed: 3.0ms preprocess, 33.5ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.1ms\n",
      "Speed: 11.4ms preprocess, 34.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.3ms\n",
      "Speed: 3.0ms preprocess, 33.3ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.6ms\n",
      "Speed: 3.3ms preprocess, 33.6ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.9ms\n",
      "Speed: 3.1ms preprocess, 33.9ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.2ms\n",
      "Speed: 3.5ms preprocess, 34.2ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.6ms\n",
      "Speed: 3.1ms preprocess, 33.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.2ms\n",
      "Speed: 2.9ms preprocess, 34.2ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.7ms\n",
      "Speed: 4.3ms preprocess, 33.7ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 33.9ms\n",
      "Speed: 2.9ms preprocess, 33.9ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.2ms\n",
      "Speed: 3.9ms preprocess, 35.2ms inference, 3.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 82.6ms\n",
      "Speed: 5.6ms preprocess, 82.6ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 82.4ms\n",
      "Speed: 6.1ms preprocess, 82.4ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 79.2ms\n",
      "Speed: 6.4ms preprocess, 79.2ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 75.9ms\n",
      "Speed: 4.0ms preprocess, 75.9ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 75.6ms\n",
      "Speed: 3.9ms preprocess, 75.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.6ms\n",
      "Speed: 3.9ms preprocess, 36.6ms inference, 3.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.5ms\n",
      "Speed: 6.4ms preprocess, 36.5ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.3ms\n",
      "Speed: 3.6ms preprocess, 35.3ms inference, 4.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 5.8ms preprocess, 35.7ms inference, 4.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.4ms\n",
      "Speed: 3.3ms preprocess, 35.4ms inference, 5.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.1ms\n",
      "Speed: 3.1ms preprocess, 36.1ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.4ms\n",
      "Speed: 4.1ms preprocess, 35.4ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.8ms\n",
      "Speed: 6.1ms preprocess, 34.8ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.8ms\n",
      "Speed: 7.5ms preprocess, 34.8ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.1ms\n",
      "Speed: 5.0ms preprocess, 35.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.9ms\n",
      "Speed: 5.6ms preprocess, 34.9ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.4ms\n",
      "Speed: 6.1ms preprocess, 35.4ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.7ms\n",
      "Speed: 2.7ms preprocess, 34.7ms inference, 3.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.3ms\n",
      "Speed: 3.0ms preprocess, 36.3ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.8ms\n",
      "Speed: 4.0ms preprocess, 34.8ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.2ms\n",
      "Speed: 3.7ms preprocess, 36.2ms inference, 2.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.8ms\n",
      "Speed: 4.0ms preprocess, 34.8ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.9ms\n",
      "Speed: 5.6ms preprocess, 34.9ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.6ms\n",
      "Speed: 2.8ms preprocess, 34.6ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.9ms\n",
      "Speed: 5.4ms preprocess, 35.9ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 34.7ms\n",
      "Speed: 3.2ms preprocess, 34.7ms inference, 3.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.5ms\n",
      "Speed: 2.9ms preprocess, 36.5ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.7ms\n",
      "Speed: 3.4ms preprocess, 36.7ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 38.5ms\n",
      "Speed: 3.4ms preprocess, 38.5ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.2ms\n",
      "Speed: 3.1ms preprocess, 35.2ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 44.7ms\n",
      "Speed: 6.7ms preprocess, 44.7ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.6ms\n",
      "Speed: 7.4ms preprocess, 36.6ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.3ms\n",
      "Speed: 2.8ms preprocess, 36.3ms inference, 2.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 37.2ms\n",
      "Speed: 2.9ms preprocess, 37.2ms inference, 2.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.2ms\n",
      "Speed: 3.8ms preprocess, 36.2ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.5ms\n",
      "Speed: 7.3ms preprocess, 36.5ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 35.7ms\n",
      "Speed: 5.6ms preprocess, 35.7ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 36.1ms\n",
      "Speed: 5.3ms preprocess, 36.1ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 (no detections), 74.2ms\n",
      "Speed: 4.8ms preprocess, 74.2ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    }
   ],
   "source": [
    "with mss.mss() as sct:\n",
    "    try:\n",
    "        cv2.namedWindow(\"YOLO Detection\", cv2.WINDOW_NORMAL)\n",
    "        # cv2.resizeWindow(\"YOLO Detection\", 1280, 720)\n",
    "\n",
    "        while True:\n",
    "            screenshot = sct.grab(bbox)\n",
    "            frame = np.array(screenshot)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "            results = model(frame)\n",
    "            annotated = results[0].plot()\n",
    "\n",
    "            cv2.imshow(\"YOLO Detection\", annotated)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"üõë Stopped by user.\")\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070f683",
   "metadata": {},
   "source": [
    "## RESNET 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b155c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maula\\AppData\\Local\\Temp\\ipykernel_32536\\391063878.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(MODEL_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚åõ Waiting for the browser window to appear...\n",
      "‚úÖ Found window: Car Control Simulation - Google Chrome\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "import pygetwindow as gw\n",
    "import time\n",
    "import mss\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"front_left\", \n",
    "    \"front_right\", \n",
    "    \"rear_left\", \n",
    "    \"rear_right\", \n",
    "    \"hood\"\n",
    "]\n",
    "\n",
    "# ====== Settings ======\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"car_door_model_new.pt\"\n",
    "URL = \"http://103.233.100.26:8080/\"\n",
    "\n",
    "# ====== Load your model ======\n",
    "model = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ====== Define preprocessing (match model input) ======\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ====== Open Browser Window ======\n",
    "webbrowser.open(URL)\n",
    "print(\"‚åõ Waiting for the browser window to appear...\")\n",
    "\n",
    "window_title = None\n",
    "while window_title is None:\n",
    "    windows = gw.getWindowsWithTitle(URL)\n",
    "    if not windows:\n",
    "        # fallback: try matching title\n",
    "        windows = [w for w in gw.getWindowsWithTitle(\" - Google Chrome\") if \"Car Control\" in w.title]\n",
    "    if windows:\n",
    "        win = windows[0]\n",
    "        window_title = win.title\n",
    "    else:\n",
    "        time.sleep(1)\n",
    "\n",
    "print(f\"‚úÖ Found window: {window_title}\")\n",
    "win = gw.getWindowsWithTitle(window_title)[0]\n",
    "\n",
    "# ====== Real-Time Inference Loop via Screen Capture ======\n",
    "with mss.mss() as sct:\n",
    "    while True:\n",
    "        bbox = {\n",
    "            \"top\": win.top,\n",
    "            \"left\": win.left,\n",
    "            \"width\": win.width,\n",
    "            \"height\": win.height\n",
    "        }\n",
    "\n",
    "        screen = sct.grab(bbox)\n",
    "        img_np = np.array(screen)\n",
    "        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "        # Preprocess\n",
    "        input_tensor = preprocess(img_np).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            if output.shape[-1] > 1:\n",
    "                preds = torch.sigmoid(output).squeeze().cpu().numpy()\n",
    "                # Show class names with confidence\n",
    "                label_str = \", \".join([\n",
    "                    f\"{CLASS_NAMES[i]}: {preds[i]:.2f}\" \n",
    "                    for i in range(len(CLASS_NAMES)) if preds[i] > 0.5\n",
    "                ]) or \"No detection\"\n",
    "            else:\n",
    "                predicted_class = torch.argmax(output, 1).item()\n",
    "                label_str = f\"Detected: {CLASS_NAMES[predicted_class]}\"\n",
    "\n",
    "        # Overlay text on image\n",
    "        cv2.putText(img_np, label_str, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"3D Object Classification (ResNet)\", img_np)\n",
    "\n",
    "        if cv2.waitKey(1) == 27:  # ESC key\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
